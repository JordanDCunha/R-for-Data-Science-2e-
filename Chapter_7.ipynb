{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvFV1KTrjuEtusUyoP60dh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/R-for-Data-Science-2e-/blob/main/Chapter_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì• **7.1 Introduction: Reading Data**\n",
        "\n",
        "Working with datasets that come bundled with R packages is a fantastic way to learn data science tools. But sooner or later, you‚Äôll want to apply those skills to **your own data**. That‚Äôs where this chapter comes in.\n",
        "\n",
        "In this chapter, you‚Äôll learn the fundamentals of **reading data files into R**, focusing specifically on **plain-text rectangular data**‚Äîthe most common and practical format you‚Äôll encounter in the real world.\n",
        "\n",
        "You‚Äôll start with hands-on guidance for dealing with common data issues, including:\n",
        "- **Column names**\n",
        "- **Column types**\n",
        "- **Missing values**\n",
        "\n",
        "From there, you‚Äôll move on to more advanced (and very useful!) workflows:\n",
        "- Reading data from **multiple files at once**\n",
        "- **Writing data** from R back to disk\n",
        "- **Handcrafting data frames** directly in R\n",
        "\n",
        "By the end of this chapter, you‚Äôll be able to confidently move data **into and out of R**, which is a critical step in any real analysis pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ **7.1.1 Prerequisites**\n",
        "\n",
        "This chapter focuses on the **readr** package, which provides fast, consistent tools for reading flat files into R.  \n",
        "`readr` is part of the **core tidyverse**, so loading the tidyverse gives you everything you need.\n",
        "\n",
        "From here on, we‚Äôll assume the tidyverse is available and ready to go.\n"
      ],
      "metadata": {
        "id": "xQjISSvu2Gym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gizocek41_bY"
      },
      "outputs": [],
      "source": [
        "library(tidyverse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ **7.2 Reading Data from a File**\n",
        "\n",
        "The most common rectangular data format you‚Äôll encounter is the **CSV file** (comma-separated values). CSV files store data in rows and columns, where:\n",
        "- The **first row** usually contains column names (the header),\n",
        "- Each **subsequent row** represents an observation,\n",
        "- Columns are **delimited by commas**.\n",
        "\n",
        "Once a CSV file exists in your project (typically inside a `data/` folder), you can read it into R using `read_csv()`. When you do, readr automatically:\n",
        "- Detects column names,\n",
        "- Guesses column types,\n",
        "- Reports potential issues (like missing values or type mismatches).\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Practical Data-Cleaning Workflow\n",
        "\n",
        "After reading data, your *very next step* is almost always cleaning and standardizing it so it‚Äôs easier to analyze. Common tasks include:\n",
        "\n",
        "- **Handling missing values**  \n",
        "  Some datasets use strings like `\"N/A\"` instead of real `NA`s. You can tell `read_csv()` exactly which values should be treated as missing.\n",
        "\n",
        "- **Fixing column names**  \n",
        "  Columns with spaces or symbols become *non-syntactic names*, requiring backticks. These are best cleaned immediately using either `rename()` or `janitor::clean_names()`.\n",
        "\n",
        "- **Correcting column types**  \n",
        "  - Categorical variables ‚Üí factors  \n",
        "  - Numeric values stored as text ‚Üí numbers  \n",
        "  - Inconsistent entries (e.g., `\"five\"` instead of `5`) ‚Üí standardized values\n",
        "\n",
        "This early cleanup prevents subtle bugs and confusion later.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Useful `read_csv()` Arguments\n",
        "\n",
        "Beyond the file path, a few arguments handle most real-world cases:\n",
        "\n",
        "- `na` ‚Äì define which strings represent missing values  \n",
        "- `skip` ‚Äì ignore the first *n* lines (metadata)  \n",
        "- `comment` ‚Äì drop lines starting with a comment character  \n",
        "- `col_names` ‚Äì specify whether headers exist (or supply your own)\n",
        "\n",
        "A neat trick: `read_csv()` can even read **inline CSV text**, which is great for examples and debugging.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ Other File Types in `readr`\n",
        "\n",
        "Once you understand `read_csv()`, the rest are easy:\n",
        "\n",
        "- `read_csv2()` ‚Äì semicolon-delimited files  \n",
        "- `read_tsv()` ‚Äì tab-delimited files  \n",
        "- `read_delim()` ‚Äì arbitrary delimiters  \n",
        "- `read_fwf()` / `read_table()` ‚Äì fixed-width files  \n",
        "- `read_log()` ‚Äì Apache-style log files  \n",
        "\n",
        "The interface stays consistent ‚Äî only the delimiter logic changes.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Takeaway\n",
        "\n",
        "Reading data is not just about importing files ‚Äî it‚Äôs about **establishing clean, reliable structure** at the very beginning. If you standardize names, fix types, and handle missing values up front, the rest of your analysis becomes dramatically easier.\n"
      ],
      "metadata": {
        "id": "9nouADas2SmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(tidyverse)\n",
        "\n",
        "# Read CSV and handle missing values\n",
        "students <- read_csv(\n",
        "  \"data/students.csv\",\n",
        "  na = c(\"N/A\", \"\")\n",
        ")\n",
        "\n",
        "# Clean names, fix types, and repair age values\n",
        "students <- students |>\n",
        "  janitor::clean_names() |>\n",
        "  mutate(\n",
        "    meal_plan = factor(meal_plan),\n",
        "    age = parse_number(if_else(age == \"five\", \"5\", age))\n",
        "  )\n",
        "\n",
        "students\n"
      ],
      "metadata": {
        "id": "w-YqgP4I2Tr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéõÔ∏è **7.3 Controlling Column Types**\n",
        "\n",
        "CSV files don‚Äôt store information about variable types, so **readr must guess** whether each column contains logicals, numbers, dates, or strings. This section explains how that guessing works, why it sometimes fails, and how to take control when needed.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 7.3.1 Guessing Types\n",
        "\n",
        "When you read a CSV, readr:\n",
        "- Samples **1,000 values per column**, evenly spaced,\n",
        "- Ignores missing values,\n",
        "- Applies a set of rules in order:\n",
        "\n",
        "1. Logical values only (`TRUE`, `FALSE`, `T`, `F`) ‚Üí logical  \n",
        "2. Numbers only (`1`, `-4.5`, `Inf`, `5e6`) ‚Üí double  \n",
        "3. ISO8601 format ‚Üí date / datetime  \n",
        "4. Otherwise ‚Üí character  \n",
        "\n",
        "This heuristic works well for clean data, but real-world data often breaks these assumptions.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è 7.3.2 Missing Values and Parsing Problems\n",
        "\n",
        "A very common failure happens when **missing values are encoded unexpectedly** (e.g. `\".\"`, `\"NA\"`, `\"NULL\"`). When that happens, readr may fall back to character type.\n",
        "\n",
        "To debug this, you can:\n",
        "1. **Force a column type** using `col_types`\n",
        "2. Inspect failures with `problems()`\n",
        "\n",
        "Once you identify the offending value, you can usually fix the issue by telling readr which strings represent missing values using the `na` argument.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± 7.3.3 Column Types You Can Specify\n",
        "\n",
        "readr provides several column type helpers, including:\n",
        "\n",
        "- `col_logical()`, `col_double()`, `col_integer()`\n",
        "- `col_character()` for IDs or codes that look numeric\n",
        "- `col_factor()`, `col_date()`, `col_datetime()`\n",
        "- `col_number()` for messy numeric data (e.g. currencies)\n",
        "- `col_skip()` to ignore columns entirely\n",
        "\n",
        "You can also:\n",
        "- Override the default guessing for *all* columns using `.default`\n",
        "- Read only selected columns using `cols_only()`\n",
        "\n",
        "Taking control of column types is especially useful when working with large or messy datasets, where silent parsing errors can lead to subtle bugs later in your analysis.\n"
      ],
      "metadata": {
        "id": "IFQT14UM2gem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(tidyverse)\n",
        "\n",
        "# Example: guessing column types\n",
        "read_csv(\"\n",
        "logical,numeric,date,string\n",
        "TRUE,1,2021-01-15,abc\n",
        "false,4.5,2021-02-15,def\n",
        "T,Inf,2021-02-16,ghi\n",
        "\")\n",
        "\n",
        "# Example: parsing problem caused by unexpected missing value\n",
        "simple_csv <- \"\n",
        "x\n",
        "10\n",
        ".\n",
        "20\n",
        "30\"\n",
        "\n",
        "df <- read_csv(\n",
        "  simple_csv,\n",
        "  col_types = list(x = col_double())\n",
        ")\n",
        "\n",
        "# Inspect parsing problems\n",
        "problems(df)\n",
        "\n",
        "# Fix by specifying missing value\n",
        "read_csv(simple_csv, na = \".\")\n",
        "\n",
        "# Override all column types\n",
        "another_csv <- \"\n",
        "x,y,z\n",
        "1,2,3\"\n",
        "\n",
        "read_csv(\n",
        "  another_csv,\n",
        "  col_types = cols(.default = col_character())\n",
        ")\n",
        "\n",
        "# Read only selected columns\n",
        "read_csv(\n",
        "  another_csv,\n",
        "  col_types = cols_only(x = col_character())\n",
        ")\n"
      ],
      "metadata": {
        "id": "iKE9brWD2hdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ **7.4 Reading Data from Multiple Files**\n",
        "\n",
        "Sometimes your data isn‚Äôt neatly stored in a single file. Instead, it‚Äôs **split across multiple files**‚Äîfor example, monthly sales data like `01-sales.csv`, `02-sales.csv`, and `03-sales.csv`. Luckily, **readr** makes it easy to read them all at once and **stack them into one tidy data frame**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Reading Multiple Files at Once\n",
        "\n",
        "If you pass a **vector of file paths** to `read_csv()`, it will:\n",
        "- Read each file,\n",
        "- Stack the rows on top of each other,\n",
        "- Return a single tibble.\n",
        "\n",
        "By using the **`id` argument**, you can also keep track of **which file each row came from**, which is extremely useful when the original files don‚Äôt contain an identifying variable.\n",
        "\n",
        "---\n",
        "\n",
        "## üåê Local Files vs URLs\n",
        "\n",
        "This approach works whether:\n",
        "- The files live in a local project directory (e.g. `data/`)\n",
        "- Or the files are hosted online and accessed via URLs\n",
        "\n",
        "In both cases, the workflow is exactly the same.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Automatically Finding Files\n",
        "\n",
        "When you have **many files**, writing them all out manually is tedious and error-prone. Instead, you can use `list.files()` to:\n",
        "- Search a directory,\n",
        "- Match file names using a **pattern**,\n",
        "- Automatically return all relevant file paths.\n",
        "\n",
        "This scales cleanly as your project grows and keeps your code flexible and reproducible.\n"
      ],
      "metadata": {
        "id": "bEN0xJzC2pZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(tidyverse)\n",
        "\n",
        "# Manually listing multiple CSV files\n",
        "sales_files <- c(\n",
        "  \"data/01-sales.csv\",\n",
        "  \"data/02-sales.csv\",\n",
        "  \"data/03-sales.csv\"\n",
        ")\n",
        "\n",
        "read_csv(sales_files, id = \"file\")\n",
        "\n",
        "# Reading the same files directly from URLs\n",
        "sales_files <- c(\n",
        "  \"https://pos.it/r4ds-01-sales\",\n",
        "  \"https://pos.it/r4ds-02-sales\",\n",
        "  \"https://pos.it/r4ds-03-sales\"\n",
        ")\n",
        "\n",
        "read_csv(sales_files, id = \"file\")\n",
        "\n",
        "# Automatically finding all sales CSV files in a directory\n",
        "sales_files <- list.files(\n",
        "  \"data\",\n",
        "  pattern = \"sales\\\\.csv$\",\n",
        "  full.names = TRUE\n",
        ")\n",
        "\n",
        "sales_files\n"
      ],
      "metadata": {
        "id": "c7QPEtF-2rMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíæ **7.5 Writing Data to a File**\n",
        "\n",
        "The **readr** package doesn‚Äôt just help you read data‚Äîit also provides tools to **write data back to disk**. This is essential for saving results, sharing data, or caching intermediate steps in your analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ Writing Plain-Text Files\n",
        "\n",
        "The two most common functions are:\n",
        "\n",
        "- **`write_csv()`** ‚Äî writes comma-separated values  \n",
        "- **`write_tsv()`** ‚Äî writes tab-separated values  \n",
        "\n",
        "The key arguments are:\n",
        "- **`x`**: the data frame to save  \n",
        "- **`file`**: the file path where the data will be written  \n",
        "\n",
        "You can also control how **missing values** are written with `na`, and whether to **append** to an existing file.\n",
        "\n",
        "‚ö†Ô∏è **Important caveat:**  \n",
        "When you save data to a CSV file, **column type information is lost**. When you read the file back in, R must guess the types again, which may not match the original object (e.g., factors becoming characters).\n",
        "\n",
        "This makes CSV files **less reliable for caching interim results** during analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Better Alternatives for Saving R Objects\n",
        "\n",
        "### üß† **RDS Files**\n",
        "- Use **`write_rds()`** and **`read_rds()`**\n",
        "- Store data in R‚Äôs native **binary format**\n",
        "- Reloading restores the **exact same R object**, including column types\n",
        "\n",
        "This is ideal for internal workflows and reproducibility.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Cross-Language, High-Performance Storage with Parquet\n",
        "\n",
        "The **arrow** package allows you to work with **Parquet files**, which are:\n",
        "- **Very fast**\n",
        "- **Compressed**\n",
        "- **Usable outside of R** (e.g., Python, Spark)\n",
        "\n",
        "Parquet combines performance with portability, but requires the **arrow** package.\n",
        "\n",
        "We‚Äôll explore this format in more detail later, but it‚Äôs a powerful option for larger or shared datasets.\n"
      ],
      "metadata": {
        "id": "ZvJd82Gb2zvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(tidyverse)\n",
        "\n",
        "# Write a CSV file\n",
        "write_csv(students, \"students.csv\")\n",
        "\n",
        "# Write another CSV and read it back in\n",
        "write_csv(students, \"students-2.csv\")\n",
        "read_csv(\"students-2.csv\")\n",
        "\n",
        "# Save and load using RDS (preserves column types)\n",
        "write_rds(students, \"students.rds\")\n",
        "read_rds(\"students.rds\")\n",
        "\n",
        "# Save and load using Parquet (fast, cross-language)\n",
        "library(arrow)\n",
        "write_parquet(students, \"students.parquet\")\n",
        "read_parquet(\"students.parquet\")\n"
      ],
      "metadata": {
        "id": "a_fQJ0gP21Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è **7.6 Data Entry**\n",
        "\n",
        "Sometimes you‚Äôll need to create a small dataset **by hand** directly in your R script. This is common for toy examples, look-up tables, or quick tests. The **tibble** package provides two especially useful functions for this, depending on whether you want to think in terms of **columns** or **rows**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Creating a Tibble by Columns with `tibble()`\n",
        "\n",
        "`tibble()` works column-by-column, similar to how you‚Äôd build a data frame in base R. Each column is defined as a vector.\n",
        "\n",
        "This approach is straightforward, but when datasets get wider, it can be harder to visually match values across rows.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Creating a Tibble by Rows with `tribble()`\n",
        "\n",
        "`tribble()` (short for **transposed tibble**) is designed specifically for **data entry in code**. Instead of defining columns as vectors, you define the data **row by row**, which often makes small datasets much easier to read.\n",
        "\n",
        "Key features of `tribble()`:\n",
        "- Column names start with `~`\n",
        "- Values are separated by commas\n",
        "- Each row appears on its own line\n",
        "\n",
        "This layout closely mirrors how we naturally read tables and is ideal for small, hand-typed datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ When to Use Each\n",
        "\n",
        "- Use **`tibble()`** when:\n",
        "  - You already have vectors\n",
        "  - You‚Äôre thinking column-wise\n",
        "\n",
        "- Use **`tribble()`** when:\n",
        "  - You‚Äôre entering data manually\n",
        "  - You want maximum readability\n"
      ],
      "metadata": {
        "id": "XAMmZKHK3BGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(tidyverse)\n",
        "\n",
        "# Column-wise data entry with tibble()\n",
        "tibble(\n",
        "  x = c(1, 2, 5),\n",
        "  y = c(\"h\", \"m\", \"g\"),\n",
        "  z = c(0.08, 0.83, 0.60)\n",
        ")\n",
        "\n",
        "# Row-wise data entry with tribble()\n",
        "tribble(\n",
        "  ~x, ~y, ~z,\n",
        "   1, \"h\", 0.08,\n",
        "   2, \"m\", 0.83,\n",
        "   5, \"g\", 0.60\n",
        ")\n"
      ],
      "metadata": {
        "id": "YjPMRaYe3CUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}